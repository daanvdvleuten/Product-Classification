{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of TFRecords (for IRNV2) and creation of image-input for ensemble (based on IRNV2 output)\n",
    "The creation of TFRecords is done by following a tutorial of Kwot Sin, which can be found here:\n",
    "###### https://kwotsin.github.io/tech/2017/01/29/tfrecords.html\n",
    "Some alterations have been made, as my approach required Stratified K-Folds, and I needed to include the unique image ID in the TFRecord. This was necessary to be able to map the IRNV2 output (probabilities) to the correct image again, as IRNV2 shuffled the batches. Thus, two modules have been changed, which have 'daan' included\n",
    "\n",
    "###### Intermediate output (TFRecords) used for \"GH - Image Classification - IRNV2 + CLR\"\n",
    "###### Intermediate output (strat_****_data) used for \"GH - Text Classification\" (and further manipulation in this notebook)\n",
    "###### Final output of this notebook used for: \"GH - Ensemble model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from dataset_utils_png import _dataset_exists, _get_filenames_and_classes, write_label_file, _convert_dataset\n",
    "from dataset_utils_png_daan import _convert_dataset_daan\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The TFRecords will be created, to make the image data suitable for the IRNV2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of Dummy Classes for the images; As normal code wants to do 'raw split' in train/test, and I want Stratified K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'C:/Users\\studentid\\Desktop\\JADS - Master Thesis\\Data\\ImagesFinalData'\n",
    "\n",
    "\n",
    "dataset_main_folder_list = [name for name in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir,name))]\n",
    "dataset_root = os.path.join(dataset_dir, dataset_main_folder_list[0])\n",
    "directories = []\n",
    "class_names = []\n",
    "\n",
    "for filename in os.listdir(dataset_root):\n",
    "    path = os.path.join(dataset_root, filename)\n",
    "    if os.path.isdir(path):\n",
    "        directories.append(path)\n",
    "        class_names.append(filename)\n",
    "\n",
    "dummy_classes_strat = []\n",
    "photo_filenames = []\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        path = os.path.join(directory, filename)\n",
    "        dummy_classes_strat.append(directory)\n",
    "        photo_filenames.append(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "#tf.app.run\n",
    "\n",
    "\n",
    "#State your dataset directory\n",
    "flags.DEFINE_string('dataset_dir', 'C:/Users\\studentid\\Desktop\\JADS - Master Thesis\\Data\\ImagesFinalData', 'String: Your dataset directory')\n",
    "\n",
    "# Proportion of dataset to be used for evaluation\n",
    "flags.DEFINE_float('validation_size', 0.25, 'Float: The proportion of examples in the dataset to be used for validation')\n",
    "\n",
    "# The number of shards to split the dataset into.\n",
    "flags.DEFINE_integer('num_shards', 10, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "# Seed for repeatability.\n",
    "flags.DEFINE_integer('random_seed', 0, 'Int: Random seed to use for repeatability.')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('tfrecord_filename', 'productimages', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "remaining_args = FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")]) #Addition, to empty flags.\n",
    "\n",
    "assert(remaining_args == [sys.argv[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The excel file 'labels_left' is used later on, to create the labelencoding based on the labels that are left.\n",
    "The 'Labels_left' file is created a few blocks below, but had to be used again here, because mapping the 'class names to ids' had to be done on the 275 labels in the data, not on the amount of labels that were present before all labels < 3 products were dropped. This is also done below, when train(1), train(2), test(1), and val(2) are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5230\n",
      "275\n"
     ]
    }
   ],
   "source": [
    "excel_labels275 = pd.read_excel('labels_left.xlsx')\n",
    "print(len(excel_labels275))\n",
    "excel_labels275 = excel_labels275.drop_duplicates()\n",
    "print(len(excel_labels275))\n",
    "list_labels275 = list(excel_labels275['labels_left'])\n",
    "#list_labels275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### photo_filenames, class_names = _get_filenames_and_classes('C:/Users\\studentid\\Desktop\\JADS - Master Thesis\\Data\\ImagesPaddedLabel')\n",
    "class_names_sorted = [int(x) for x in list_labels275]\n",
    "class_names_sorted.sort(key=float)\n",
    "class_names_string = [str(item) for item in class_names_sorted]\n",
    "class_names_to_ids = dict(zip(class_names_string, range(len(class_names_string))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_names_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of train(1), test(1), train(2), and val(2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The first split is made here. \n",
    "#The complete dataset is split in train(1) and test(1)\n",
    "#Hereafter, test(1) has to be split in train(2) and val(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_1, test_1, y_train_1, y_test_1 = train_test_split(photo_filenames, dummy_classes_strat,\n",
    "                                                    stratify= dummy_classes_strat, \n",
    "                                                    test_size=0.25, random_state = 1)\n",
    "\n",
    "#The creation of train(1) is now complete. This set does not need further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16068\n",
      "5356\n"
     ]
    }
   ],
   "source": [
    "print(len(train_1))\n",
    "print(len(test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, the classes with less than 4 products have to be dropped again\n",
    "#To allow for gridsearch and a correct stratified split\n",
    "test_1_df = pd.DataFrame({'image_test': test_1})\n",
    "y_test_1_df = pd.DataFrame({'test_label': y_test_1})\n",
    "total_test_1 = pd.concat([test_1_df, y_test_1_df], axis=1)\n",
    "\n",
    "#Drop the labels with too few products\n",
    "#Test_1 will not be used in the end, only its resulting datasets train(2) and val(2)\n",
    "counts = total_test_1['test_label'].value_counts()\n",
    "total_test_1 = total_test_1[total_test_1['test_label'].isin(counts[counts > 3].index)]\n",
    "total_test_1 = total_test_1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This has led to a reduction \n",
    "total_test_1['test_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creation of second split\n",
    "#Here, the test set created above is used as 'total dataset', and is split in a new train and val set.\n",
    "\n",
    "train_2, val_2, y_train_2, y_val_2 = train_test_split(total_test_1['image_test'], total_test_1['test_label'],\n",
    "                                                    stratify= total_test_1['test_label'], \n",
    "                                                    test_size=0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3922\n",
      "1308\n"
     ]
    }
   ],
   "source": [
    "print(len(train_2))\n",
    "print(len(val_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure the data formats are equal again (train_2 and val_2 are different compared to train-1)\n",
    "train_2 = list(train_2)\n",
    "val_2 = list(val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Necessary preprocessing to create image-ids, later on needed\n",
    "unique_images_train1 = []\n",
    "for image1 in train_1:\n",
    "    single_image1 = os.path.basename((image1))\n",
    "    unique_images_train1.append(single_image1)\n",
    "\n",
    "unique_images_train2 = []\n",
    "for image2 in train_2:\n",
    "    single_image2 = os.path.basename((image2))\n",
    "    unique_images_train2.append(single_image2)    \n",
    "    \n",
    "unique_images_val2 = []\n",
    "for image3 in val_2:\n",
    "    single_image3 = os.path.basename((image3))\n",
    "    unique_images_val2.append(single_image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16068\n",
      "3922\n",
      "1308\n"
     ]
    }
   ],
   "source": [
    "#Necessary processing to create dictionaries with the image_ids\n",
    "unique_images_to_ids_train1 = dict(zip(unique_images_train1, range(len(unique_images_train1))))\n",
    "unique_images_to_ids_train2 = dict(zip(unique_images_train2, range(len(unique_images_train2))))\n",
    "unique_images_to_ids_val2 = dict(zip(unique_images_val2, range(len(unique_images_val2))))\n",
    "\n",
    "#Check if lengths are still correct\n",
    "print(len(unique_images_to_ids_train1))\n",
    "print(len(unique_images_to_ids_train2))\n",
    "print(len(unique_images_to_ids_val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, write the labels file:\n",
    "labels_to_class_names = dict(zip(range(len(class_names_string)), class_names_string))\n",
    "write_label_file(labels_to_class_names, FLAGS.dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to make sure train/validation sets are same for text and images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have splitted the text data and image data separately from each other, we have to make sure both base-models (text classification & image classification) have the same training data, and also same validation data! \n",
    "Or else the meta-classifier will not be able to classify..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a copy of train_1, train_2 & val_2\n",
    "#So they can afterwards be converted to new dataframes\n",
    "#To match with the textdata, for creating the same text & image sets\n",
    "copy_train_1 = train_1[:]\n",
    "copy_train_2 = train_2[:]\n",
    "copy_val_2 = val_2[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, a match has to be made between the training images and the 'textdata' file, to make a separation in that data (based on the stratified split made here). The same goes for the validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "image_files_train1 = pd.DataFrame({'image_train1': copy_train_1})\n",
    "image_files_train2 = pd.DataFrame({'image_train2': copy_train_2})\n",
    "image_files_validation2 = pd.DataFrame({'image_validation2': copy_val_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the format each string of images has, below is what it needs to be\n",
    "image_files_validation2['image_validation2'][26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the 'imagedata.xlsx' , the file that will be matched against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totaldata = pd.read_excel('imagedata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All the '/' are replaced by '\\\\', now the strings are equal\n",
    "image_files_train1['image_train1'] = image_files_train1['image_train1'].str.replace('/','\\\\')\n",
    "image_files_train2['image_train2'] = image_files_train2['image_train2'].str.replace('/','\\\\')\n",
    "image_files_validation2['image_validation2'] = image_files_validation2['image_validation2'].str.replace('/','\\\\')\n",
    "\n",
    "totaldata['Images'] = totaldata['Images'].str.replace('/','\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The strings still differ, as the image-data was kept in two layers deeper folders.\n",
    "#These parts of the string have to be ommitted.\n",
    "\n",
    "edited_train_string1 = []\n",
    "for row1 in image_files_train1['image_train1']:\n",
    "    part1 = row1[:57]\n",
    "    part2 = row1[-15:]\n",
    "    total1 = part1 + part2\n",
    "    edited_train_string1.append(total1)\n",
    "    \n",
    "edited_train_string2 = []\n",
    "for row2 in image_files_train2['image_train2']:\n",
    "    part3 = row2[:57]\n",
    "    part4 = row2[-15:]\n",
    "    total2 = part3 + part4\n",
    "    edited_train_string2.append(total2)\n",
    "    \n",
    "edited_val_string2 = []\n",
    "for row3 in image_files_validation2['image_validation2']:\n",
    "    part_a = row3[:57]\n",
    "    part_b = row3[-15:]\n",
    "    total_ab = part_a + part_b\n",
    "    edited_val_string2.append(total_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write new DF, with the strings edited. Now the two df's can be matched\n",
    "image_files_train_edit1 = pd.DataFrame({'image_train_edit1': edited_train_string1})\n",
    "image_files_train_edit2 = pd.DataFrame({'image_train_edit2': edited_train_string2})\n",
    "image_files_val_edit2 = pd.DataFrame({'image_val_edit2': edited_val_string2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The two new datasets are created, these will be used in both 'Text Classification\n",
    "#and 'Image Classification (the latter already created with TFRecords)\n",
    "\n",
    "strat_train1_data = pd.merge(left=image_files_train_edit1, right=totaldata, left_on=image_files_train_edit1['image_train_edit1'], right_on=totaldata['Images'])\n",
    "strat_train2_data = pd.merge(left=image_files_train_edit2, right=totaldata, left_on=image_files_train_edit2['image_train_edit2'], right_on=totaldata['Images'])\n",
    "\n",
    "strat_val2_data = pd.merge(left=image_files_val_edit2, right=totaldata, left_on=image_files_val_edit2['image_val_edit2'], right_on=totaldata['Images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_left = []\n",
    "for label in total_test_1['test_label']:\n",
    "    pathofimage = label[84:]\n",
    "    total_label = pathofimage\n",
    "    labels_left.append(total_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_left_df = pd.DataFrame({'labels_left': labels_left})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_left_df.to_excel(\"labels_left.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN1 needs to have same amount of classes as TRAIN2 and VAL2\n",
    "# Because InceptionResNet V2 requires same amount of classes in train/test\n",
    "# So a new merge, to make sure they have same labels.\n",
    "# The old 'directory structure', where images are saved, is required again\n",
    "# Simply concatenate them, as structure is retained with earlier merge\n",
    "strat_train1_data = pd.concat([strat_train1_data, image_files_train1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here, all label differences are dropped. \n",
    "\n",
    "unique_labels_left = pd.DataFrame({'Label':labels_left_df['labels_left'].unique()})\n",
    "unique_labels_left['Label'] = pd.to_numeric(unique_labels_left['Label'])\n",
    "strat_train1_data = strat_train1_data[strat_train1_data['Label'].isin(unique_labels_left['Label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#strat_train1_data['image_train1'][0]\n",
    "new_train_1 = list(strat_train1_data['image_train1'])\n",
    "\n",
    "#Get unique ids\n",
    "unique_images_new_train1 = []\n",
    "for new_image1 in new_train_1:\n",
    "    single_image = os.path.basename((new_image1))\n",
    "    unique_images_new_train1.append(single_image)\n",
    "    \n",
    "unique_images_to_ids_new_train1 = dict(zip(unique_images_new_train1, range(len(unique_images_new_train1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train1 has been slightly reduced, now the new TFRecord is created.\n",
    "len(strat_train1_data)\n",
    "\n",
    "#Now the TFRecords file is written\n",
    "_convert_dataset_daan('train', new_train_1, class_names_to_ids, unique_images_to_ids_new_train1,\n",
    "                 dataset_dir = FLAGS.dataset_dir,\n",
    "                 tfrecord_filename = FLAGS.tfrecord_filename,\n",
    "                 _NUM_SHARDS = FLAGS.num_shards)\n",
    "\n",
    "#Train2\n",
    "_convert_dataset_daan('traintest', train_2, class_names_to_ids, unique_images_to_ids_train2,\n",
    "                 dataset_dir = FLAGS.dataset_dir,\n",
    "                 tfrecord_filename = FLAGS.tfrecord_filename,\n",
    "                 _NUM_SHARDS = FLAGS.num_shards)\n",
    "\n",
    "#Val_2\n",
    "_convert_dataset_daan('validation', val_2, class_names_to_ids, unique_images_to_ids_val2,     \n",
    "                 dataset_dir = FLAGS.dataset_dir,\n",
    "                 tfrecord_filename = FLAGS.tfrecord_filename,\n",
    "                 _NUM_SHARDS = FLAGS.num_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally, all the datasets have been created.\n",
    "\n",
    "strat_train1_data.to_excel(\"strat_train1_data.xlsx\", index=False)\n",
    "strat_train2_data.to_excel(\"strat_train2_data.xlsx\", index=False)\n",
    "strat_val2_data.to_excel(\"strat_val2_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#strat_val2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! The two files created here are sent to the Jupyter Notebook 'Text Classification'. There these two files will be used as the training & validation data (train is split into train/test inside the GridSearch algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last section in this notebook: rematch 'image_id' output of InceptionResNetV2 with the actual images\n",
    "#### This is the case for train2 and val2 (train2 is traindata for svm, val2 is val data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000018772.png\n"
     ]
    }
   ],
   "source": [
    "#For each of the images in 'inception_output.xlsx', the 'image_id' will be rematched with its actual '.png' file. \n",
    "#This is done to be able to perfectly match the 'text_output' and 'image_output', to make them serve as input for final model.\n",
    "\n",
    "print(list(unique_images_to_ids_val2.keys())[list(unique_images_to_ids_val2.values()).index(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The output of the InceptionResNetV2 model, as preprocessed in 'Preprocess Image Output.ipynb', is imported\n",
    "inception_output_train2 = pd.read_excel('inception-output_train2.xlsx')\n",
    "inception_output_val2 = pd.read_excel('inception-output_val2.xlsx')\n",
    "\n",
    "# CREATE: Inception_output_train2 and Inception_output_val2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rematched_images_train2 = []\n",
    "for row in inception_output_train2['ImageID']:\n",
    "    pngname1 = list(unique_images_to_ids_train2.keys())[list(unique_images_to_ids_train2.values()).index(row)]\n",
    "    rematched_images_train2.append(pngname1)\n",
    "\n",
    "rematched_images_val2 = []\n",
    "for row in inception_output_val2['ImageID']:\n",
    "    pngname2 = list(unique_images_to_ids_val2.keys())[list(unique_images_to_ids_val2.values()).index(row)]\n",
    "    rematched_images_val2.append(pngname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rematched_images_val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basepath_train2 = []\n",
    "for row1 in image_files_train2['image_train2']:\n",
    "    pathofimage1 = row1[:57]\n",
    "    dashsign1 = '/'\n",
    "    total1 = pathofimage1 + dashsign1\n",
    "    basepath_train2.append(total1)\n",
    "\n",
    "basepath_val2 = []\n",
    "for row2 in image_files_validation2['image_validation2']:\n",
    "    pathofimage2 = row2[:57]\n",
    "    dashsign2 = '/'\n",
    "    total2 = pathofimage2 + dashsign2\n",
    "    basepath_val2.append(total2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3922"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rematched_images_train2)\n",
    "len(basepath_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create DF's for both the train2 and val2 dataset\n",
    "basepath_train2_df = pd.DataFrame({'basepath_train2': basepath_train2})\n",
    "rematched_images_train2_df = pd.DataFrame({'pngnames_train2': rematched_images_train2})\n",
    "rematched_images_train2_df['matched_images'] = basepath_train2_df['basepath_train2'] + '' + rematched_images_train2_df['pngnames_train2']\n",
    "rematched_images_train2_df['matched_images'] = rematched_images_train2_df['matched_images'].str.replace('/', '\\\\')\n",
    "\n",
    "basepath_val2_df = pd.DataFrame({'basepath_val2': basepath_val2})\n",
    "rematched_images_val2_df = pd.DataFrame({'pngnames_val2': rematched_images_val2})\n",
    "rematched_images_val2_df['matched_images'] = basepath_val2_df['basepath_val2'] + '' + rematched_images_val2_df['pngnames_val2']\n",
    "rematched_images_val2_df['matched_images'] = rematched_images_val2_df['matched_images'].str.replace('/', '\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rematched_images_df['matched_images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3922"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inception_output_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inception_output_train2 = pd.concat([inception_output_train2, rematched_images_train2_df], axis=1)\n",
    "\n",
    "inception_output_val2 = pd.concat([inception_output_val2, rematched_images_val2_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train2_dataset['matched_images'][0]\n",
    "#inception_output_val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train2_dataset = pd.read_excel('strat_train2_data.xlsx')\n",
    "val2_dataset = pd.read_excel('strat_val2_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val2_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3922\n"
     ]
    }
   ],
   "source": [
    "matches_counter = 0\n",
    "\n",
    "for row in train2_dataset['image_train_edit2']:\n",
    "    for image in inception_output_train2['matched_images']:\n",
    "        if row == image:\n",
    "            matches_counter += 1\n",
    "\n",
    "#CHECK IF ALL IMAGES MATCH IN BOTH DATASETS, AND THEY DO! \n",
    "print(matches_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The two frames are now merged, to create the dataframe that will be used as input for the final layer of the ensemble model.\n",
    "image_input_train2_ensemble = train2_dataset.merge(inception_output_train2, left_on='image_train_edit2', right_on='matched_images', how='inner')\n",
    "\n",
    "image_input_val2_ensemble = val2_dataset.merge(inception_output_val2, left_on='image_val_edit2', right_on='matched_images', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1308"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_input_val2_ensemble)\n",
    "#image_input_val2_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image_input_val2_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some unnecessary columns are dropped, before writing this to an excel file.\n",
    "#What is important to remain in the data, are the probability scores, and their respective labels\n",
    "cols = [1,2,3,4,5,6,7,8,9,10,12,17,18]\n",
    "\n",
    "image_input_train2_ensemble.drop(image_input_train2_ensemble.columns[cols],axis=1,inplace=True)\n",
    "\n",
    "image_input_val2_ensemble.drop(image_input_val2_ensemble.columns[cols],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image_input_val2_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As the labels were changed (due to stratification in train/test(1) and train/val(2), the label 'names' have to be remapped)\n",
    "#The 'Predictions' and 'Labels' columns, resulting from the Inception model, still have the encoded labels, instead of the actual\n",
    "#That's why there is a mismatch at the moment between 'Label' and 'Labels'.\n",
    "ids_to_class_names = {v: k for k, v in class_names_to_ids.items()}\n",
    "\n",
    "image_input_train2_ensemble = image_input_train2_ensemble.replace({\"Labels\": ids_to_class_names})\n",
    "image_input_train2_ensemble = image_input_train2_ensemble.replace({\"Predictions\": ids_to_class_names})\n",
    "\n",
    "image_input_val2_ensemble = image_input_val2_ensemble.replace({\"Labels\": ids_to_class_names})\n",
    "image_input_val2_ensemble = image_input_val2_ensemble.replace({\"Predictions\": ids_to_class_names})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remapping is a success!\n",
    "#image_input_val2_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finally, write the dataframe to Excel. This file will later on be used, in combination with the text output.\n",
    "image_input_train2_ensemble.to_excel(\"image_input_train2_ensemble.xlsx\", header=True)\n",
    "\n",
    "image_input_val2_ensemble.to_excel(\"image_input_val2_ensemble.xlsx\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
