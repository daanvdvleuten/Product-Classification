{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Classification: Training and testing of Inception ResNet V2, using CLR for learning rate\n",
    "The code for implementing Inception ResNet V2 using a form of transfer learning is based on the code by Kwot Sin: \n",
    "###### https://kwotsin.github.io/tech/2017/02/11/transfer-learning.html\n",
    "Using this tutorial, the initial IRNV2 code was created. There have been several alterations in the code, with the most important example changing the learning rate 'model'. For this, Cyclical Learning Rate (CLR) as proposed in the paper by Smith (2017): \"Cyclical learning rates for training neural networks\".\n",
    "Other alterations include saving the predicted probabilities of the model, instead of only an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "import inception_preprocessing\n",
    "import inception_daan_preprocessing\n",
    "import learning_rate_decay_daan\n",
    "from learning_rate_decay_daan import cyclic_lr\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "import os\n",
    "import time\n",
    "slim = tf.contrib.slim\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting point: State directories & load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#State the labels file and read it\n",
    "labels_file = 'C:/Users\\studentid\\Desktop\\JADS - Master Thesis\\Data\\ImagesFinalData/labels.txt'\n",
    "labels = open(labels_file, 'r')\n",
    "\n",
    "#State image size\n",
    "image_size = 299\n",
    "\n",
    "#Create a dictionary to refer each label to their string name\n",
    "labels_to_name = {}\n",
    "for line in labels:\n",
    "    label, string_name = line.split(':')\n",
    "    string_name = string_name[:-1] #Remove newline\n",
    "    labels_to_name[int(label)] = string_name\n",
    "\n",
    "#Create the file pattern of your TFRecord files so that it could be recognized later on\n",
    "file_pattern = 'productimages_%s_*.tfrecord'\n",
    "\n",
    "#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\n",
    "items_to_descriptions = {\n",
    "    'image': 'Image of a product in the dataset, out of 275 classes',\n",
    "    'image_id': 'Unique identifier for each image',\n",
    "    'label': 'A label that matches the class of the image.'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training & Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#============== DATASET LOADING ======================\n",
    "#We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "#     '''\n",
    "#     Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "#     set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "#     Your file_pattern is very important in locating the files later. \n",
    "\n",
    "#     INPUTS:\n",
    "#     - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "#     - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "#     - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "#     - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n",
    "\n",
    "#     OUTPUTS:\n",
    "#     - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "#     '''\n",
    "\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']:\n",
    "        raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/id': tf.FixedLenFeature((), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "    'image': slim.tfexample_decoder.Image(),\n",
    "    'image_id': slim.tfexample_decoder.Tensor('image/id'),\n",
    "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        #num_readers = 4,\n",
    "        num_samples = 15678, #Number of training data\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: Loading the training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_min = 24)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, image_id, label = data_provider.get(['image', 'image_id','label'])\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_daan_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, image_ids, labels = tf.train.batch(\n",
    "        [image, raw_image, image_id, label],\n",
    "        batch_size = batch_size,\n",
    "        num_threads = 4,\n",
    "        capacity = 4 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, image_ids, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING THE COMPLETE MODEL CODE BELOW\n",
    "\n",
    "### FIRST: TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#================ DATASET INFORMATION ======================\n",
    "#State dataset directory where the tfrecord files are located\n",
    "dataset_dir = 'C:/Users\\studentid\\Desktop\\JADS - Master Thesis\\Data\\ImagesPaddedLabel275'\n",
    "\n",
    "#State where your log file is at. If it doesn't exist, create it.\n",
    "log_dir = 'C:/Users\\studentid\\Desktop\\JADS - Master Thesis\\Data\\ImagesPaddedLabel275/TRAIN-FINAL'\n",
    "\n",
    "#State where your checkpoint file is\n",
    "checkpoint_file = 'C:/Users\\studentid\\Desktop\\JADS - Master Thesis\\Data/inception_resnet_v2_2016_08_30.ckpt'\n",
    "\n",
    "#State the image size you're resizing your images to. We will use the default inception size of 299.\n",
    "image_size = 299\n",
    "\n",
    "#State the number of classes to predict:\n",
    "num_classes = 275 \n",
    "\n",
    "#State the labels file and read it\n",
    "labels_file = 'C:/Users\\studentid\\Desktop\\JADS - Master Thesis\\Data\\ImagesFinalData/labels.txt'\n",
    "labels = open(labels_file, 'r')\n",
    "\n",
    "#Create a dictionary to refer each label to their string name\n",
    "labels_to_name = {}\n",
    "for line in labels:\n",
    "    label, string_name = line.split(':')\n",
    "    string_name = string_name[:-1] #Remove newline\n",
    "    labels_to_name[int(label)] = string_name\n",
    "\n",
    "#Create the file pattern of your TFRecord files so that it could be recognized later on\n",
    "file_pattern = 'productimages_%s_*.tfrecord'\n",
    "\n",
    "#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\n",
    "items_to_descriptions = {\n",
    "    'image': 'Image of a product in the dataset, out of 275 classes',\n",
    "    'image_id': 'Unique identifier for each image',\n",
    "    'label': 'A label that matches the class of the image.'\n",
    "}\n",
    "\n",
    "\n",
    "#================= TRAINING INFORMATION ==================\n",
    "#State the number of epochs to train\n",
    "num_epochs = 40 #After a while, model is converged, and stops improving.\n",
    "\n",
    "#State your batch size\n",
    "batch_size = 13 \n",
    "\n",
    "base_lr = 0.0001\n",
    "max_lr = 0.005\n",
    "stepsize = 1206 #to find optimal values (stepsize x nr epochs = stepsize) (Known as: Learning Rate Range Test)\n",
    "#Normal training: stepsize = 3x num_iterations ; Thus then stepsize is 3618.\n",
    "\n",
    "#============== DATASET LOADING ======================\n",
    "#We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later. \n",
    "\n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "    - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n",
    "\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']:\n",
    "        raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    #reader = tf.TFRecordReader\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/id': tf.FixedLenFeature((), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "    'image': slim.tfexample_decoder.Image(),\n",
    "    'image_id': slim.tfexample_decoder.Tensor('image/id'),\n",
    "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_readers = 4,\n",
    "        num_samples = 16068, #15678\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_min = 24)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, label, image_id = data_provider.get(['image', 'label', 'image_id'])\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_daan_preprocessing.preprocess_image(raw_image, height, width, is_training) #Edited preprocessing\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, labels, image_ids = tf.train.batch(\n",
    "        [image, raw_image, label, image_id],\n",
    "        batch_size = batch_size,\n",
    "        num_threads = 4,\n",
    "        capacity = 4 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, labels, image_ids\n",
    "\n",
    "def run():\n",
    "    \n",
    "    # Create lists for accuracy vs learning_rate plot\n",
    "    acc_list = []\n",
    "    lr_list = []\n",
    "    \n",
    "    #Create the log directory here. Must be done here otherwise import will activate this unneededly.\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)  \n",
    "\n",
    "\n",
    "    #======================= TRAINING PROCESS =========================\n",
    "    #Now we start to construct the graph and build our model\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.logging.set_verbosity(tf.logging.INFO) #Set the verbosity to INFO level\n",
    "        \n",
    "        #First create the dataset and load one batch\n",
    "        dataset = get_split('train', dataset_dir, file_pattern=file_pattern)\n",
    "        images, _, labels, image_ids = load_batch(dataset, batch_size=batch_size)\n",
    "\n",
    "        #Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "        num_batches_per_epoch = math.ceil(dataset.num_samples / batch_size) ## Used to be int(dataset...../batch..)\n",
    "        num_steps_per_epoch = num_batches_per_epoch #Because one step is one batch processed\n",
    "\n",
    "        #Create the model inference\n",
    "        with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "            logits, end_points = inception_resnet_v2(images, num_classes = dataset.num_classes, is_training = True)\n",
    "\n",
    "        #Define the scopes that you want to exclude for restoration\n",
    "        exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\n",
    "        variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "\n",
    "        #Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\n",
    "        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "\n",
    "        #Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
    "        total_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well\n",
    "\n",
    "        #Create the global step for monitoring the learning_rate and training.\n",
    "        global_step = get_or_create_global_step()\n",
    "        \n",
    "        \n",
    "        #Created an implementation of Cyclic LR, instead of exponential decay\n",
    "        lr1 = learning_rate_decay_daan.cyclic_lr(0.001, global_step, stepsize, base_lr, max_lr)\n",
    "\n",
    "        #Define your exponentially decaying learning rate #This is the old learning_rate model.\n",
    "#         lr = tf.train.exponential_decay(\n",
    "#             learning_rate = initial_learning_rate,\n",
    "#             global_step = global_step,\n",
    "#             decay_steps = decay_steps,\n",
    "#             decay_rate = learning_rate_decay_factor,\n",
    "#             staircase = True)\n",
    "\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = lr1)\n",
    "\n",
    "        #Create the train_op.\n",
    "        train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "        #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "        predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "        probabilities = end_points['Predictions']\n",
    "        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "        metrics_op = tf.group(accuracy_update, probabilities)\n",
    "        \n",
    "\n",
    "        #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "        tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.scalar('learning_rate', lr1)\n",
    "        my_summary_op = tf.summary.merge_all()\n",
    "\n",
    "        #Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n",
    "        def train_step(sess, train_op, global_step):\n",
    "            '''\n",
    "            Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n",
    "            '''\n",
    "            #Check the time for each sess run\n",
    "            start_time = time.time()\n",
    "            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            #Run the logging to print some results\n",
    "            logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
    "\n",
    "            return total_loss, global_step_count\n",
    "\n",
    "        #Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        def restore_fn(sess):\n",
    "            return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        #Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n",
    "        sv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\n",
    "        \n",
    "        \n",
    "        def plotlist(sess, acc, lr):\n",
    "        \n",
    "            acc_value, lr_value = sess.run([acc, lr])\n",
    "\n",
    "            return acc_value, lr_value\n",
    "            \n",
    "        \n",
    "        #Run the managed session\n",
    "        with sv.managed_session() as sess:\n",
    "\n",
    "            for step in range(num_steps_per_epoch * num_epochs):\n",
    "                #At the start of every epoch, show the vital information:\n",
    "                if step % num_batches_per_epoch == 0:\n",
    "                    logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, num_epochs)\n",
    "                    learning_rate_value, accuracy_value = sess.run([lr1, accuracy])\n",
    "                    logging.info('Current Learning Rate: %s', learning_rate_value)\n",
    "                    logging.info('Current Streaming Accuracy: %s', accuracy_value)\n",
    "\n",
    "                    # optionally, print your logits and predictions for a sanity check that things are going fine.\n",
    "                    logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n",
    "                    print('logits: \\n'), logits_value\n",
    "                    print('Probabilities: \\n'), probabilities_value\n",
    "                    print('predictions: \\n'), predictions_value\n",
    "                    print('Labels:\\n:'), labels_value\n",
    "\n",
    "                #Store accuracy value & learning rate value every step during the training.\n",
    "                #This is for LR Range Test (see below)\n",
    "                \n",
    "                #Log the summaries every 10 step.\n",
    "                if step % 10 == 0:\n",
    "                    loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "                    summaries = sess.run(my_summary_op)\n",
    "                    sv.summary_computed(sess, summaries)\n",
    "                    accstore, lrstore = plotlist(sess, accuracy, lr1)\n",
    "                    acc_list.append(accstore)\n",
    "                    lr_list.append(lrstore)\n",
    "                \n",
    "#                 if step == 10:\n",
    "#                     break\n",
    "                    \n",
    "                #If not, simply run the training step\n",
    "                else:\n",
    "                    loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "                    accstore, lrstore = plotlist(sess, accuracy, lr1)\n",
    "                    acc_list.append(accstore)\n",
    "                    lr_list.append(lrstore)\n",
    "                    \n",
    "                \n",
    "            \n",
    "            #We log the final training loss and accuracy\n",
    "            logging.info('Final Loss: %s', loss)\n",
    "            logging.info('Final Accuracy: %s', sess.run(accuracy))\n",
    "            \n",
    "            #Just to be sure, write the acc_list and lr_list to a csv, to also get back to these in normal python scripts.\n",
    "            outfile_acc = open('LR-rangetest-acc.csv','w')\n",
    "            out_acc = csv.writer(outfile_acc)\n",
    "            out_acc.writerows(map(lambda x: [x], acc_list))\n",
    "            outfile_acc.close()\n",
    "            \n",
    "            outfile_lr = open('LR-rangetest-lr.csv','w')\n",
    "            out_lr = csv.writer(outfile_lr)\n",
    "            out_lr.writerows(map(lambda x: [x], lr_list))\n",
    "            outfile_lr.close()\n",
    "            \n",
    "            fig3, ax3 = plt.subplots()\n",
    "            ax3.plot(lr_list, acc_list, color='b', linestyle ='solid')\n",
    "            ax3.autoscale(enable=True, axis=\"y\", tight=False)\n",
    "            \n",
    "            plt.xlabel('Learning Rate')\n",
    "            plt.ylabel('Accuracy Value')\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "            #Once all the training has been done, save the log files and checkpoint model\n",
    "            logging.info('Finished training! Saving model to disk now.')\n",
    "            sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n",
    "\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on new data ; EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "import inception_preprocessing\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "import time\n",
    "import os\n",
    "#from train_flowers import get_split, load_batch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "slim = tf.contrib.slim\n",
    "import csv\n",
    "\n",
    "#State your log directory where you can retrieve your model\n",
    "log_dir = 'C:/Users\\s164677\\Desktop\\JADS - Master Thesis\\Data\\ImagesPaddedLabel\\log1'\n",
    "\n",
    "#Create a new evaluation log directory to visualize the validation process\n",
    "log_eval = 'C:/Users\\s164677\\Desktop\\JADS - Master Thesis\\Data\\ImagesPaddedLabel/log_eval_test'\n",
    "\n",
    "#State the dataset directory where the validation set is found\n",
    "dataset_dir = 'C:/Users\\s164677\\Desktop\\JADS - Master Thesis\\Data\\ImagesPaddedLabel'\n",
    "\n",
    "#State the batch_size to evaluate each time, which can be a lot more than the training batch\n",
    "batch_size = 26\n",
    "\n",
    "#State the number of classes\n",
    "num_classes = 275\n",
    "\n",
    "#State the number of epochs to evaluate\n",
    "num_epochs = 10 #Model stops improving after +- 3 epochs.\n",
    "\n",
    "#Get the latest checkpoint file\n",
    "checkpoint_file = tf.train.latest_checkpoint(log_dir)\n",
    "\n",
    "#============== DATASET LOADING ======================\n",
    "#We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later. \n",
    "\n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "    - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n",
    "\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']:\n",
    "        raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/id': tf.FixedLenFeature((), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "    'image': slim.tfexample_decoder.Image(),\n",
    "    'image_id': slim.tfexample_decoder.Tensor('image/id'),\n",
    "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_readers = 4,\n",
    "        shuffle=False,\n",
    "        num_samples = 5344, #Amount of training dataset (2), validation dataset (2) number also goes here.\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=False):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, shuffle=False,\n",
    "        common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_min = 24)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, label, image_id = data_provider.get(['image', 'label', 'image_id'])\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_daan_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, labels, image_ids = tf.train.batch(\n",
    "        [image, raw_image, label, image_id],\n",
    "        batch_size = batch_size,\n",
    "        num_threads = 1, #To prevent shuffling the output, which makes it unable to use ensemble later on. Threads = 1.\n",
    "        capacity = 4 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, labels, image_ids\n",
    "\n",
    "\n",
    "def run():\n",
    "    \n",
    "    probabilities_list = []\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "    id_list = []\n",
    "    #Create log_dir for evaluation information\n",
    "    if not os.path.exists(log_eval):\n",
    "        os.mkdir(log_eval)\n",
    "\n",
    "    #Just construct the graph from scratch again\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "        #Get the dataset first and load one batch of validation images and labels tensors. Set is_training as False so as to use the evaluation preprocessing\n",
    "        dataset = get_split('validation', dataset_dir)\n",
    "        images, raw_images, labels, image_ids = load_batch(dataset, batch_size = batch_size, is_training = False)\n",
    "\n",
    "        #Create some information about the training steps\n",
    "        num_batches_per_epoch = (dataset.num_samples / batch_size)\n",
    "        num_steps_per_epoch = num_batches_per_epoch\n",
    "        print('steps per epoch: %s' %(num_steps_per_epoch))\n",
    "        print('number of test images: %s' %(dataset.num_samples))\n",
    "        \n",
    "\n",
    "        #Now create the inference model but set is_training=False\n",
    "        with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "            logits, end_points = inception_resnet_v2(images, num_classes = dataset.num_classes, is_training = False)\n",
    "\n",
    "        # #get all the variables to restore from the checkpoint file and create the saver function to restore\n",
    "        variables_to_restore = slim.get_variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        def restore_fn(sess):\n",
    "            return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        #Just define the metrics to track without the loss or whatsoever\n",
    "        predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "        \n",
    "        \n",
    "        probabilities = end_points['Predictions']\n",
    "        \n",
    "        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "        metrics_op = tf.group(accuracy_update)\n",
    "\n",
    "        #Create the global step and an increment op for monitoring\n",
    "        global_step = get_or_create_global_step()\n",
    "        global_step_op = tf.assign(global_step, global_step + 1) #no apply_gradient method so manually increasing the global_step\n",
    "        \n",
    "\n",
    "        #Create a evaluation step function\n",
    "        def eval_step(sess, metrics_op, global_step):\n",
    "#             '''\n",
    "#             Simply takes in a session, runs the metrics op and some logging information.\n",
    "#             '''\n",
    "            start_time = time.time()\n",
    "            _, global_step_count, accuracy_value = sess.run([metrics_op, global_step_op, accuracy])\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            #Log some information\n",
    "            logging.info('Global Step %s: Streaming Accuracy: %.4f (%.2f sec/step)', global_step_count, accuracy_value, time_elapsed)\n",
    "\n",
    "            return accuracy_value\n",
    "\n",
    "        \n",
    "        def store_lists(sess, probabilities, image_ids, labels, predictions):\n",
    "            prob_batch, ids_batch, labels_batch, pred_batch = sess.run([probabilities, image_ids, labels, predictions])\n",
    "            \n",
    "            return prob_batch, ids_batch, labels_batch, pred_batch\n",
    "        \n",
    "\n",
    "        #Define some scalar quantities to monitor\n",
    "        tf.summary.scalar('Validation_Accuracy', accuracy)\n",
    "        my_summary_op = tf.summary.merge_all()\n",
    "\n",
    "        #Get your supervisor\n",
    "        sv = tf.train.Supervisor(logdir = log_eval, summary_op = None, saver = None, init_fn = restore_fn)\n",
    "\n",
    "        #Now we are ready to run in one session\n",
    "        with sv.managed_session() as sess:\n",
    "            for step in range(int((num_steps_per_epoch * num_epochs))):  ### HAD TO MAKE INT OF IT, GAVE FLOAT ERROR               \n",
    "                sess.run(sv.global_step)\n",
    "                             \n",
    "                #Get all necessary values to append to lists\n",
    "                prob_store, id_store, label_store, pred_store = store_lists(sess, probabilities, image_ids, labels, predictions)\n",
    "                probabilities_list.append(prob_store)\n",
    "                id_list.append(id_store)\n",
    "                label_list.append(label_store)\n",
    "                pred_list.append(pred_store)\n",
    "                \n",
    "                #print vital information every start of the epoch as always\n",
    "                if step % num_batches_per_epoch == 0:\n",
    "                    logging.info('Epoch: %s/%s', step / num_batches_per_epoch + 1, num_epochs)\n",
    "                    logging.info('Current Streaming Accuracy: %.4f', sess.run(accuracy))\n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                #Compute summaries every 10 steps and continue evaluating\n",
    "                if step % 10 == 0:\n",
    "                    eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n",
    "                    summaries = sess.run(my_summary_op)\n",
    "                    sv.summary_computed(sess, summaries)\n",
    "                    \n",
    "                    #Get all necessary values to append to lists\n",
    "                    prob_store, id_store, label_store, pred_store = store_lists(sess, probabilities, image_ids, labels, predictions)\n",
    "                    probabilities_list.append(prob_store)\n",
    "                    id_list.append(id_store)\n",
    "                    label_list.append(label_store)\n",
    "                    pred_list.append(pred_store)\n",
    "\n",
    "\n",
    "                #Otherwise just run as per normal\n",
    "                else:\n",
    "                    eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n",
    "                    \n",
    "                    #Get all necessary values to append to lists\n",
    "                    prob_store, id_store, label_store, pred_store = store_lists(sess, probabilities, image_ids, labels, predictions)\n",
    "                    probabilities_list.append(prob_store)\n",
    "                    id_list.append(id_store)\n",
    "                    label_list.append(label_store)\n",
    "                    pred_list.append(pred_store)\n",
    "    \n",
    "            \n",
    "            #At the end of all the evaluation, show the final accuracy\n",
    "            logging.info('Final Streaming Accuracy: %.4f', sess.run(accuracy)) \n",
    "            \n",
    "         \n",
    "        \n",
    "            #And write these to a csv\n",
    "            id_list = np.asarray(id_list) ## ID is necessary, to map all probabilities to correct images!\n",
    "            #Because IRNV2 swaps the batches, the order is then shuffled. So a unique identifier is necessary.\n",
    "            with open('output-imageid.csv', 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerows(id_list)\n",
    "            \n",
    "            probabilities_list = np.asarray(probabilities_list) #The predicted probabilities\n",
    "            with open('output-probabilities.csv', 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerows(probabilities_list)\n",
    "            \n",
    "            label_list = np.asarray(label_list) #The correct labels\n",
    "            with open('output-labels.csv', 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerows(label_list)\n",
    "            \n",
    "            pred_list = np.asarray(pred_list) #The predicted labels\n",
    "            with open ('output-predictions.csv', 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerows(pred_list)\n",
    "            \n",
    "\n",
    "            #Now we want to visualize the last batch's images just to see what our model has predicted\n",
    "            raw_images, labels, predictions = sess.run([raw_images, labels, predictions])\n",
    "            for i in range(10):\n",
    "                image, label, prediction = raw_images[i], labels[i], predictions[i]\n",
    "                prediction_name, label_name = dataset.labels_to_name[prediction], dataset.labels_to_name[label]\n",
    "                text = 'Prediction: %s \\n Ground Truth: %s' %(prediction_name, label_name)\n",
    "                img_plot = plt.imshow(image)\n",
    "\n",
    "                #Set up the plot and hide axes\n",
    "                plt.title(text)\n",
    "                img_plot.axes.get_yaxis().set_ticks([])\n",
    "                img_plot.axes.get_xaxis().set_ticks([])\n",
    "                plt.show()\n",
    "\n",
    "            logging.info('Model evaluation has completed! Visit TensorBoard for more information regarding your evaluation.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagestf",
   "language": "python",
   "name": "imagestf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
