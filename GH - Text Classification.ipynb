{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the text classification task is performed. First the 3 datasets are imported, then 'text preprocessing' is applied (detailed info in Master Thesis). Once this has been done, all of the five chosen models are trained and tested, after finding the optimal parameters.\n",
    "\n",
    "Then all these predictions are saved, as they are the input for the meta-learner. These files are used in \"GH - Ensemble Model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from stop_words import get_stop_words\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import 'training dataset(1)', 'training dataset(2)', and 'validation dataset(2)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = pd.read_excel(\"strat_train1_data.xlsx\")\n",
    "train2 = pd.read_excel(\"strat_train2_data.xlsx\")\n",
    "val2 = pd.read_excel(\"strat_val2_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_preprocess(dataset):\n",
    "    dataset = dataset.fillna('nan')\n",
    "    dataset['longdescription'] = dataset['longdescription'].str.replace('nan', '')\n",
    "    dataset['keywords'] = dataset['keywords'].str.replace('nan', '')\n",
    "    dataset['text_input'] = dataset['shortdescription'] + ' ' + dataset['longdescription'].fillna('') + ' ' + dataset['namemanufacturer'] + ' ' + dataset['keywords'].fillna('')\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Apply all text preprocessing to the datasets\n",
    "train1 = text_preprocess(train1)\n",
    "train2 = text_preprocess(train2)\n",
    "val2 = text_preprocess(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create corpus of the training and validation data\n",
    "\n",
    "def create_corpus(listname, dataset):\n",
    "    stop_words1 = get_stop_words('german')\n",
    "\n",
    "    for i in range(0,len(dataset.text_input)):\n",
    "        review1 = re.sub('[^a-zA-ZüäöØ°C0-9()ß-]',' ', str(dataset.text_input[i])) ## ADDED ö, Diameter, Degree Celsius\n",
    "        review1 = review1.lower()\n",
    "        review1 = review1.split()\n",
    "        stemmer1 = SnowballStemmer(\"german\")\n",
    "        review1 = [stemmer1.stem(word) for word in review1 if not word in set(stop_words1)]\n",
    "        review1 =  ' '.join(review1)\n",
    "        listname.append(review1)\n",
    "        \n",
    "    return listname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_train1 = []\n",
    "corpus_train2 = []\n",
    "corpus_val2 = []\n",
    "\n",
    "corpus_train1 = create_corpus(corpus_train1, train1)\n",
    "corpus_train2 = create_corpus(corpus_train2, train2)\n",
    "corpus_val2 = create_corpus(corpus_val2, val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 50000)\n",
    "X_train1 = cv.fit_transform(corpus_train1)\n",
    "\n",
    "#train2 and val2 only need to be transformed, as they are both subsets of train1\n",
    "X_train2 = cv.transform(corpus_train2)\n",
    "X_val2 = cv.transform(corpus_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15678, 35581)\n",
      "(3922, 35581)\n",
      "(1308, 35581)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s164677\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "X_train1 = tfidf.fit_transform(X_train1)\n",
    "\n",
    "#Here as well, only transformation needed.\n",
    "X_train2 = tfidf.transform(X_train2)\n",
    "X_val2 = tfidf.transform(X_val2)\n",
    "print(X_train1.shape)\n",
    "print(X_train2.shape)\n",
    "print(X_val2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15678,)\n",
      "(3922,)\n",
      "(1308,)\n"
     ]
    }
   ],
   "source": [
    "Y_train1 = train1.Label\n",
    "Y_train2 = train2.Label\n",
    "Y_val2 = val2.Label\n",
    "print(Y_train1.shape)\n",
    "print(Y_train2.shape)\n",
    "print(Y_val2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: GridSearch of models & train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of Stratified K-folds has been done in 'Create_TFRecords.ipynb', as the image split was done there as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN model, gridsearch to find optimal parameters first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values_n = range(1,20)\n",
    "weight_options = ['uniform', 'distance']\n",
    "param_grid1 = dict(n_neighbors = values_n, weights = weight_options)\n",
    "\n",
    "sk_fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "clf = KNeighborsClassifier()\n",
    "grid = GridSearchCV(clf, param_grid1, cv=sk_fold, verbose=3, scoring = 'accuracy')\n",
    "grid.fit(X_train1, Y_train1)\n",
    "#grid.cv_results_\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the optimal parameters, found in GridSearch, to fit and predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc train2:  0.9472208057113718\n",
      "Acc val2:  0.9457186544342507\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "#Optimal params: n_neighbors = 2, weights = 'distance'\n",
    "knn = KNeighborsClassifier(n_neighbors = 2, weights = 'distance')\n",
    "knn.fit(X_train1, Y_train1)\n",
    "\n",
    "knn_predicted_train2 = knn.predict(X_train2)\n",
    "knn_predicted_val2 = knn.predict(X_val2)\n",
    "\n",
    "print(\"Acc train2: \", accuracy_score(Y_train2, knn_predicted_train2))\n",
    "print(\"Acc val2: \", accuracy_score(Y_val2, knn_predicted_val2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_proba_train2 = knn.predict_proba(X_train2)\n",
    "knn_proba_val2 = knn.predict_proba(X_val2)\n",
    "\n",
    "knn_predlabel_train2 = pd.DataFrame(knn_predicted_train2)\n",
    "knn_predlabel_val2 = pd.DataFrame(knn_predicted_val2)\n",
    "knn_df_train2 = pd.DataFrame(knn_proba_train2)\n",
    "knn_df_val2 = pd.DataFrame(knn_proba_val2)\n",
    "\n",
    "knn_predlabel_train2.to_excel(\"KNN_labelpredictions_train2.xlsx\")\n",
    "knn_predlabel_val2.to_excel(\"KNN_labelpredictions_val2.xlsx\")\n",
    "knn_df_train2.to_excel(\"KNN_Predictions_train2.xlsx\")\n",
    "knn_df_val2.to_excel(\"KNN_Predictions_val2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LogReg with OneVsRest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_logreg = {\n",
    "    \"estimator__solver\": ['liblinear', 'lbfgs', 'newton-cg', 'sag'], ## lbfgs, newton-cg, sag are inferior to LIBLINEAR\n",
    "    \"estimator__multi_class\": [\"ovr\"],\n",
    "    \"estimator__C\": [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "sk_logreg = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "clf_logreg = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "grid = GridSearchCV(clf_logreg, p_logreg, cv=sk_logreg)\n",
    "grid.fit(X_train1, Y_train1)\n",
    "grid.cv_results_\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc train2:  0.9553799082100969\n",
      "Acc val2:  0.9548929663608563\n"
     ]
    }
   ],
   "source": [
    "# OPTIMAL PARAMS: C=100, multi_class = 'ovr', solver = 'liblinear'\n",
    "lr_ovr = OneVsRestClassifier(LogisticRegression(solver='liblinear', multi_class = 'ovr', C=100)) ##CHECK, MODELS ARE THE SAME!\n",
    "lr_ovr.fit(X_train1, Y_train1)\n",
    "\n",
    "lr_pred_train2 = lr_ovr.predict(X_train2)\n",
    "lr_pred_val2 = lr_ovr.predict(X_val2)\n",
    "print(\"Acc train2: \", accuracy_score(Y_train2, lr_pred_train2))\n",
    "print(\"Acc val2: \", accuracy_score(Y_val2, lr_pred_val2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_proba_train2 = lr_ovr.predict_proba(X_train2)\n",
    "lr_proba_val2 = lr_ovr.predict_proba(X_val2)\n",
    "\n",
    "lr_df_train2 = pd.DataFrame(lr_proba_train2)\n",
    "lr_df_val2 = pd.DataFrame(lr_proba_val2)\n",
    "\n",
    "lr_df_train2.to_excel(\"LR_Predictions_train2.xlsx\")\n",
    "lr_df_val2.to_excel(\"LR_Predictions_val2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naïve Bayes\n",
    "No grid search is applicable to MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc train2:  0.8220295767465579\n",
      "Acc: 0.8241590214067278\n"
     ]
    }
   ],
   "source": [
    "nb_ovr = OneVsRestClassifier(MultinomialNB())\n",
    "nb_ovr.fit(X_train1, Y_train1)\n",
    "\n",
    "nb_pred_train2 = nb_ovr.predict(X_train2)\n",
    "nb_pred_val2 = nb_ovr.predict(X_val2)\n",
    "\n",
    "print(\"Acc train2: \", accuracy_score(Y_train2, nb_pred_train2))\n",
    "print(\"Acc:\", accuracy_score(Y_val2, nb_pred_val2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_proba_train2 = nb_ovr.predict_proba(X_train2)\n",
    "nb_proba_val2 = nb_ovr.predict_proba(X_val2)\n",
    "\n",
    "nb_df_train2 = pd.DataFrame(nb_proba_train2)\n",
    "nb_df_val2 = pd.DataFrame(nb_proba_val2)\n",
    "\n",
    "nb_df_train2.to_excel(\"NB_Predictions_train2.xlsx\")\n",
    "nb_df_val2.to_excel(\"NB_Predictions_val2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC model\n",
    "When kernel is set to linear, gamma has no influence.\n",
    "GridSearch runs for a couple of hours (>15 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_svc = {\n",
    "    \"estimator__C\": [0.001, 0.01, 0.1, 1, 10, 100], ##Eerst stond hier: 1,10,100,1000\n",
    "    \"estimator__kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"] ## AND RBF POLY SIGMOID\n",
    "    \"estimator__gamma\":[0.001, 0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sk_SVC = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "clf3 = OneVsRestClassifier(SVC())\n",
    "\n",
    "grid = GridSearchCV(clf3, p_svc, cv=sk_SVC, verbose=3)\n",
    "grid.fit(X_train1, Y_train1)\n",
    "grid.cv_results_\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc train2:  0.9576746557878634\n",
      "Acc val2: 0.9541284403669725\n"
     ]
    }
   ],
   "source": [
    "#Optimal params: C=10, kernel= 'linear', gamma = 1\n",
    "svc_ovr = OneVsRestClassifier(SVC(C=10 , kernel='linear', gamma=1, probability=False))\n",
    "svc_ovr.fit(X_train1, Y_train1)\n",
    "\n",
    "svc_predicted_train2 = svc_ovr.predict(X_train2)\n",
    "svc_predicted_val2 = svc_ovr.predict(X_val2)\n",
    "\n",
    "print(\"Acc train2: \", accuracy_score(Y_train2, svc_predicted_train2))\n",
    "print(\"Acc val2:\", accuracy_score(Y_val2, svc_predicted_val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_predlabel_train2 = pd.DataFrame(svc_predicted_train2)\n",
    "svc_predlabel_val2 = pd.DataFrame(svc_predicted_val2)\n",
    "\n",
    "svc_predlabel_train2.to_excel(\"SVC_labelpredictions_train2.xlsx\")\n",
    "svc_predlabel_val2.to_excel(\"SVC_labelpredictions_val2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_proba_train2 = svc_ovr.predict_proba(X_train2)\n",
    "svc_proba_val2 = svc_ovr.predict_proba(X_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_df_train2 = pd.DataFrame(svc_proba_train2)\n",
    "svc_df_val2 = pd.DataFrame(svc_proba_val2)\n",
    "\n",
    "svc_df_train2.to_excel(\"SVC_Predictions_train2.xlsx\")\n",
    "svc_df_val2.to_excel(\"SVC_Predictions_val2.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree Classifier\n",
    "Runs for approx. 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"estimator__criterion\": ['gini', 'entropy'],\n",
    "    \"estimator__splitter\":['best', 'random'],\n",
    "}\n",
    "\n",
    "\n",
    "sk_DT = StratifiedKFold(n_splits=3, shuffle=True, random_state=4)\n",
    "\n",
    "clf4 = OneVsRestClassifier(DecisionTreeClassifier())\n",
    "\n",
    "grid4 = GridSearchCV(clf4, parameters, cv=sk_DT, n_jobs=-1, verbose=3)\n",
    "grid4.fit(X_train1, Y_train1)\n",
    "grid4.cv_results_\n",
    "grid4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc train2: 0.8908720040795512\n",
      "Acc val2: 0.8876146788990825\n"
     ]
    }
   ],
   "source": [
    "#Optimal params: criterion = 'entropy', splitter = 'random'\n",
    "\n",
    "dt_ovr = OneVsRestClassifier(DecisionTreeClassifier(criterion = 'entropy', splitter = 'random'))\n",
    "dt_ovr.fit(X_train1, Y_train1)\n",
    "\n",
    "dt_pred_train2 = dt_ovr.predict(X_train2)\n",
    "dt_pred_val2 = dt_ovr.predict(X_val2)\n",
    "\n",
    "print(\"Acc train2:\", accuracy_score(Y_train2, dt_pred_train2))\n",
    "print(\"Acc val2:\", accuracy_score(Y_val2, dt_pred_val2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_proba_train2 = dt_ovr.predict_proba(X_train2)\n",
    "dt_proba_val2 = dt_ovr.predict_proba(X_val2)\n",
    "\n",
    "dt_df_train2 = pd.DataFrame(dt_proba_train2)\n",
    "dt_df_val2 = pd.DataFrame(dt_proba_val2)\n",
    "\n",
    "dt_df_train2.to_excel(\"DT_Predictions_train2.xlsx\")\n",
    "dt_df_val2.to_excel(\"DT_Predictions_val2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics calculation\n",
    "Metrics revelant: Accuracy (F1-Micro) and F1-Macro\n",
    "Accuracy & F1-Micro are supposed to be equal (multi-class setting), thus check is performed.\n",
    "\n",
    "Further below, Accuracy (F1-Micro) and F1-Macro are calculated for IRNV2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metrics_calc(pred):\n",
    "    print(\"Acc: \", accuracy_score(Y_val2, pred))\n",
    "    print(\"F1-micro: \", f1_score(Y_val2, pred, average = 'micro'))\n",
    "    print(\"F1-macro: \", f1_score(Y_val2, pred, average = 'macro'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"KNN:\")\n",
    "metrics_calc(knn_predicted_val2)\n",
    "print(\"LR:\")\n",
    "metrics_calc(lr_pred_val2)\n",
    "print(\"NB:\")\n",
    "metrics_calc(nb_pred_val2)\n",
    "print(\"SVM:\")\n",
    "metrics_calc(svc_predicted_val2)\n",
    "print(\"DT:\")\n",
    "metrics_calc(dt_pred_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CODE COPIED FROM \"Create TFRecords ipynb\", to map the labels according to dictionary values (needed to make top-5 acc)\n",
    "excel_labels275 = pd.read_excel('labels_left.xlsx')\n",
    "#print(len(excel_labels275))\n",
    "excel_labels275 = excel_labels275.drop_duplicates()\n",
    "#print(len(excel_labels275))\n",
    "list_labels275 = list(excel_labels275['labels_left'])\n",
    "\n",
    "class_names_sorted = [int(x) for x in list_labels275]\n",
    "class_names_sorted.sort(key=float)\n",
    "class_names_string = [str(item) for item in class_names_sorted]\n",
    "class_names_to_ids = dict(zip(class_names_string, range(len(class_names_string))))\n",
    "#class_names_to_ids\n",
    "\n",
    "#The mapping has to be done other way around, in this case\n",
    "ids_to_class_names = {v: k for k, v in class_names_to_ids.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 1 Accuracy Inception ResNet V2 (Image model) & F1 micro/F1 macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_val_input = pd.read_excel('image_input_val2_ensemble.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_val_input['Probabilities'] = image_val_input['Probabilities'].str.replace('[', '')\n",
    "image_val_input['Probabilities'] = image_val_input['Probabilities'].str.replace(']', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image_probabilities = image_probabilities.values.tolist()\n",
    "image_val_prob = []\n",
    "for row1 in image_val_input['Probabilities']:\n",
    "    probclass1 = row1.split()\n",
    "    probclass1 = list(map(float, probclass1))\n",
    "    image_val_prob.append(probclass1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_val_prob = np.array(image_val_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top1and5_acc_image(probability_predictions, ids_to_class_names):\n",
    "    \n",
    "    df_proba = pd.DataFrame(probability_predictions)\n",
    "    prediction_top_proba = []\n",
    "    for index, row in df_proba.iterrows():\n",
    "        df_proba = df_proba = df_proba.rename(columns=ids_to_class_names)\n",
    "        top_predictions = df_proba.sort_values(by=index, axis=1, ascending = False).columns.values[0:5]\n",
    "        prediction_top_proba.append(top_predictions)\n",
    "\n",
    "    prediction_top_proba = [list(map(int, row)) for row in prediction_top_proba]\n",
    "    predictions_top_df = pd.DataFrame({'Y_val2': Y_val2, 'Y_top_proba': prediction_top_proba })\n",
    "    total_1 = 0\n",
    "    total_5 = 0\n",
    "    for index, row in predictions_top_df.iterrows():\n",
    "        real = predictions_top_df['Y_val2'].loc[index]\n",
    "        predicted_list = predictions_top_df['Y_top_proba'].loc[index]\n",
    "        true_pred_1 = 0\n",
    "        true_pred_5 = 0\n",
    "        \n",
    "        for prediction_1 in predicted_list[0:1]:\n",
    "            if real == prediction_1:\n",
    "                true_pred_1 = 1\n",
    "        total_1 += true_pred_1\n",
    "        \n",
    "        for prediction_5 in predicted_list[0:5]:\n",
    "            if real == prediction_5:\n",
    "                true_pred_5 = 1\n",
    "        total_5 += true_pred_5\n",
    "    \n",
    "    top1_acc = total_1/len(predictions_top_df)\n",
    "    top5_acc = total_5/len(predictions_top_df)\n",
    "    print(\"Top-1 accuracy %.2f\"%(top1_acc*100))\n",
    "    print('Top-5 Accuracy %.2f'%(top5_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top1and5_acc_image(image_val_prob, ids_to_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Obtain the list of predicted labels from the image dataset, to calculate F1 micro & F1 macro\n",
    "## The function prints the associated F1 micro & F1 macro scores right away\n",
    "def f1micromacro(probability_predictions, ids_to_class_names):\n",
    "    predicted_labels = []\n",
    "    \n",
    "    df_proba = pd.DataFrame(probability_predictions)\n",
    "    prediction_top_proba = []\n",
    "    for index, row in df_proba.iterrows():\n",
    "        df_proba = df_proba = df_proba.rename(columns=ids_to_class_names)\n",
    "        top_predictions = df_proba.sort_values(by=index, axis=1, ascending = False).columns.values[0:5]\n",
    "        prediction_top_proba.append(top_predictions)\n",
    "\n",
    "    prediction_top_proba = [list(map(int, row)) for row in prediction_top_proba]\n",
    "    predictions_top_df = pd.DataFrame({'Y_val2': Y_val2, 'Y_top_proba': prediction_top_proba })\n",
    "    total_1 = 0\n",
    "    total_5 = 0\n",
    "    for index, row in predictions_top_df.iterrows():\n",
    "        real = predictions_top_df['Y_val2'].loc[index]\n",
    "        predicted_list = predictions_top_df['Y_top_proba'].loc[index]\n",
    "        true_pred_1 = 0\n",
    "        true_pred_5 = 0\n",
    "        \n",
    "        for prediction_1 in predicted_list[0:1]:\n",
    "            predicted_labels.append(prediction_1)\n",
    "            \n",
    "            \n",
    "    print(\"F1-micro: \", f1_score(Y_val2, predicted_labels, average = 'micro'))\n",
    "    print(\"F1-macro: \", f1_score(Y_val2, predicted_labels, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1micromacro(image_val_prob, ids_to_class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
